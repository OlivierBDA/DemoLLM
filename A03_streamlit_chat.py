import os
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ==============================================================================
# Demo LLM - Phase A : √âtape 3 : Interface Graphique (Streamlit)
# ==============================================================================
# Ce programme offre une interface web professionnelle pour la conversation.
# ASPECT CL√â : S√©paration stricte entre le "Core LLM" et l'interface "Streamlit".
# ==============================================================================
# python -m streamlit run 03_streamlit_chat.py

# ------------------------------------------------------------------------------
# SECTION 1 : LOGIQUE COEUR LLM (LangChain)
# ------------------------------------------------------------------------------

def init_llm():
    """Charge la config et initialise le client LLM agnostique."""
    load_dotenv()
    return ChatOpenAI(
        model=os.getenv("LLM_MODEL"),
        api_key=os.getenv("LLM_API_KEY"),
        base_url=os.getenv("LLM_BASE_URL"),
        temperature=0.7,
        streaming=True
    )

def get_session_starter_messages():
    """D√©finit le contexte initial de la conversation."""
    return [
        SystemMessage(content="Tu es un assistant expert de l'univers Marvel. Tu fais des r√©ponses courtes et concises.")
    ]

def add_message_to_history(history, role, text):
    """Ajoute dynamiquement un message √† la structure de donn√©es LangChain."""
    if role == "user":
        history.append(HumanMessage(content=text))
    elif role == "assistant":
        history.append(AIMessage(content=text))
    return history

def get_llm_response_stream(llm, message_history):
    """D√©clenche l'appel au LLM et retourne le flux de streaming (g√©n√©rateur)."""
    # ASPECT CL√â : C'est ici que la continuit√© se fait en envoyant l'historique complet.
    return llm.stream(message_history)


# ------------------------------------------------------------------------------
# SECTION 2 : INTERFACE UTILISATEUR (Streamlit)
# ------------------------------------------------------------------------------

def configure_page():
    """Configure les m√©ta-donn√©es et le style de la page."""
    st.set_page_config(page_title="Demo LLM - Marvel UI", page_icon="ü¶∏", layout="wide")
    st.title("ü¶∏ Demo LLM : Assistant Marvel")
    
    # ENCART D'INFORMATION
    with st.expander("‚ÑπÔ∏è √Ä propos de cette √©tape : Premi√®re Interface", expanded=False):
        st.markdown("""
        **Concept : Sortir du terminal**
        Cette √©tape introduit **Streamlit**, un framework qui permet de transformer des scripts Python en applications web interactives.
        
        **Fonctionnement :**
        - L'interface g√®re l'historique de session (`st.session_state`).
        - Le LLM est appel√© en mode **Streaming** pour une exp√©rience plus fluide.
        """)
        st.graphviz_chart('''
            digraph G {
                rankdir=LR;
                node [shape=box, fontname="Helvetica", fontsize=10];
                User [label="Utilisateur"];
                UI [label="Streamlit Chat", style=filled, color=lightblue];
                LLM [label="LLM (OpenAI/Gemini)", style=filled, color=orange];
                
                User -> UI [label="Question"];
                UI -> LLM [label="Historique + Prompt"];
                LLM -> UI [label="R√©ponse (Stream)"];
            }
        ''')
    st.markdown("---")

def render_sidebar():
    """Affiche les param√®tres techniques dans la barre lat√©rale."""
    with st.sidebar:
        st.title("‚öôÔ∏è Configuration")
        load_dotenv()
        st.info(f"**Mod√®le :** {os.getenv('LLM_MODEL')}\n\n**Endpoint :** {os.getenv('LLM_BASE_URL')}")
        
        if st.button("üóëÔ∏è Nouvelle Conversation", use_container_width=True):
            # R√©initialisation via le Core LLM
            st.session_state.messages = get_session_starter_messages()
            st.rerun()

def render_chat_history():
    """Affiche les messages stock√©s en ignorant les messages syst√®me."""
    for msg in st.session_state.messages:
        if isinstance(msg, HumanMessage):
            with st.chat_message("user"):
                st.markdown(msg.content)
        elif isinstance(msg, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(msg.content)

def handle_user_interaction(llm):
    """G√®re la saisie utilisateur et la r√©ponse stream√©e de l'IA."""
    if prompt := st.chat_input("Posez votre question sur les super-h√©ros..."):
        # UI : Affichage utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Core : Mise √† jour historique
        st.session_state.messages = add_message_to_history(st.session_state.messages, "user", prompt)

        # UI & Core : R√©ponse en streaming
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""
            
            try:
                # Appel au g√©n√©rateur du Core LLM
                for chunk in get_llm_response_stream(llm, st.session_state.messages):
                    full_response += chunk.content
                    placeholder.markdown(full_response + "‚ñå")
                
                placeholder.markdown(full_response)
                
                # Core : M√©morisation de la r√©ponse AI
                st.session_state.messages = add_message_to_history(st.session_state.messages, "assistant", full_response)
                
            except Exception as e:
                st.error(f"Erreur LLM : {e}")

def main():
    # 1. Configuration Initiale
    configure_page()
    render_sidebar()
    
    # 2. Initialisation du State (Core LLM)
    if "messages" not in st.session_state:
        st.session_state.messages = get_session_starter_messages()
    
    # 3. Initialisation du LLM (Core LLM)
    llm_client = init_llm()

    # 4. Rendu de l'interface
    render_chat_history()
    handle_user_interaction(llm_client)

if __name__ == "__main__":
    main()
