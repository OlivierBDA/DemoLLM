import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ==============================================================================
# ü¶∏ Demo LLM - Phase A : √âtape 2 : Conversation en Terminal
# ==============================================================================
# Ce programme permet d'avoir une conversation interactive avec le LLM.
# ASPECT CL√â : Contrairement √† l'√©tape 1, nous conservons la liste des messages
# √©chang√©s pour que le LLM ait le "contexte" de la discussion en cours.
# ==============================================================================

def main():
    # Chargement des variables d'environnement
    load_dotenv()
    
    model_name = os.getenv("LLM_MODEL")
    api_key = os.getenv("LLM_API_KEY")
    base_url = os.getenv("LLM_BASE_URL")
    
    if not api_key or not model_name:
        print("Erreur : Configuration manquante dans le fichier .env.")
        return

    # Initialisation du mod√®le (Interface Standard)
    llm = ChatOpenAI(
        model=model_name,
        api_key=api_key,
        base_url=base_url,
        temperature=0.7
    )

    # ASPECT CL√â : Initialisation de la m√©moire de session (liste de messages)
    # On commence par un message syst√®me pour d√©finir le comportement de l'IA.
    messages = [
        SystemMessage(content="Tu es un assistant expert de l'univers Marvel. Tu r√©ponds de mani√®re pr√©cise et enthousiaste. Tu fais des r√©ponses courtes et concises.")
    ]

    print("--- Demo LLM - √âtape 2 : Mode Chat Interactif ---")
    print("(Tapez 'exit' ou 'quitter' pour arr√™ter la conversation)")
    
    while True:
        # 1. Saisie utilisateur
        user_input = input("\nVous : ")
        
        if user_input.lower() in ["exit", "quitter", "quit"]:
            print("Fin de la conversation. √Ä bient√¥t !")
            break
            
        if not user_input.strip():
            continue

        # 2. Ajout du message utilisateur √† l'historique de la session
        # ASPECT CL√â : On accumule les messages dans la liste 'messages'
        messages.append(HumanMessage(content=user_input))
        
        print(f"\n[Attente de la r√©ponse du mod√®le '{model_name}'...]")
        
        try:
            # 3. Appel du mod√®le avec toute la continuit√© des messages
            # ASPECT CL√â : On passe LA LISTE COMPL√àTE au LLM
            response = llm.invoke(messages)
            
            # 4. Affichage et m√©morisation de la r√©ponse de l'IA
            print("\nAI : " + response.content)
            
            # ASPECT CL√â : Il faut aussi m√©moriser la r√©ponse de l'AI pour le prochain tour
            messages.append(AIMessage(content=response.content))
            
        except Exception as e:
            print(f"\nUne erreur est survenue : {e}")
            break

if __name__ == "__main__":
    main()
